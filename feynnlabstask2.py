# -*- coding: utf-8 -*-
"""FeynnLabsTask2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GavATQJ8MVt7X6Q-n6j1wOFMaboZRVrN

**Importing Libraries**
"""

!pip install bioinfokit

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from bioinfokit.visuz import cluster
from sklearn.cluster import KMeans
from yellowbrick.cluster import KElbowVisualizer
from collections import Counter
from statsmodels.graphics.mosaicplot import mosaic
from itertools import product
import warnings
warnings.filterwarnings('ignore')

"""**Loading the Dataset**"""

uploaded = files.upload()

df = pd.read_csv(io.BytesIO(uploaded.get('mcdonalds.csv')))

df

pd.pandas.set_option('Display.max_columns',None)
pd.pandas.set_option('Display.max_rows',None)

"""**Data Cleansing and Exploratory Data Analysis**"""

# first five record of the dataset
df.head()

# last five record of the dataset
df.tail()

# shape of the dataset
df.shape

# features of the dataset
df.columns

df['Gender'].value_counts()

df['VisitFrequency'].value_counts()

df['Like'].value_counts()

# information about the dataset
df.info()

df.dtypes

# summary  statistics of the dataset
df.describe()

# summary statistics of the categorical features
df.describe(include='object')

# check null values
df.isnull().sum()

# Handling Duplicate Value

df.duplicated().sum()

df.drop_duplicates(inplace=True)
print(df.duplicated().sum())

from pandas.plotting import boxplot
boxplot(df['Age'])

# There is no outlier in the dataset

plt.figure(figsize=(12,6))
sns.countplot(x=df['Age'])
plt.xlabel("Age")
plt.ylabel("Count of Person")
plt.title("Age Distribution of Customers")
plt.show()

"""Obsevation: presence of 55-year old person is more than the other."""

plt.rcParams['figure.figsize'] = (7, 7)
plt.pie(df['Gender'].value_counts(), colors = ['cyan','purple'],  labels = ['Female','Male'], shadow = True, autopct = '%.2f%%')
plt.title('Gender', fontsize = 10)
plt.axis('off')
plt.legend()
plt.show()

"""**Data Pre-Processing**"""

# Drop columns
dfNew = df.drop(labels=['Like','Age','VisitFrequency','Gender'], axis=1)

dfNew.head()

# label encoder maps 'No' to 0 and 'Yes' to 1
dfEncoded = dfNew.apply(LabelEncoder().fit_transform)
dfEncoded.head()

"""**Applying Principal Component Analysis**"""

# applying scaling and PCA
pca_data = preprocessing.scale(dfEncoded)
pca = PCA(n_components=11)
pc = pca.fit_transform(pca_data)
names = ['pc1','pc2','pc3','pc4','pc5','pc6','pc7','pc8','pc9','pc10','pc11']
pf = pd.DataFrame(data = pc, columns = names)
pf.head()

loadings = pca.components_
num_pc = pca.n_features_
pc_list = ["PC"+str(i) for i in list(range(1, num_pc+1))]
loadings_df = pd.DataFrame.from_dict(dict(zip(pc_list, loadings)))
loadings_df['feature'] = dfEncoded.columns.values
loadings_df = loadings_df.set_index('feature')
loadings_df

"""**Interpretation**: The column PC1 indicates how the first principal component is composed of the original variables. For the first principal component tasty, yummy and disgusting are important variables. For the second principal component cheap and expensive are important variables. For the third principal component fattening, greasy, healthy, convenient are important variables."""

# correlation matrix plot of principal components
plt.rcParams['figure.figsize'] = (15,12)
ax = sns.heatmap(loadings_df, annot=True, cmap='Purples')
plt.show()

# get PC scores
pca_scores = PCA().fit_transform(pca_data)

# get 2D biplot
cluster.biplot(cscore=pca_scores, loadings=loadings, labels=df.columns.values,
               var1=round(pca.explained_variance_ratio_[0]*100, 2),
    var2=round(pca.explained_variance_ratio_[1]*100, 2),show=True,dim=(10,5))

"""**Interpretation**: The above figure shows how the principal components are composed of the original values. As we can see expensive is quite unique. A group of respondents are in the middle

**Applying the Elbow Method**
"""

# using k-means clustering analysis for extracting segments
model = KMeans()
visualizer = KElbowVisualizer(model, k=(1,12)).fit(dfEncoded)
visualizer.show();

"""**Applying Clustering Algorithms**"""

# k-means clustering
kmeans = KMeans(n_clusters=4, init='k-means++', random_state=0).fit(dfEncoded)
df['cluster_num'] = kmeans.labels_ #adding to df
# label assigned for each data point
print ('Labels:', kmeans.labels_)
# gives within-cluster sum of squares (WCSS)
print ('WCSS:', kmeans.inertia_)
# number of iterations that k-means algorithm runs to get a minimum within-cluster sum of squares
print('No. of iterations: ', kmeans.n_iter_)
# location of the centroids on each cluster
print('Cluster centroids: ', kmeans.cluster_centers_)
# checking each cluster size
print('Cluster size: ', Counter(kmeans.labels_))

# cluster visualization
kmeans = KMeans(n_clusters=4, init='k-means++', random_state=0).fit(dfEncoded)
sns.scatterplot(data=pf, x="pc1", y="pc2", hue=kmeans.labels_)
plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], marker="X", c="r", s=80, label="centroids")
plt.legend()
plt.show()

df['cluster_num'] = kmeans.labels_
crosstab = pd.crosstab(df['cluster_num'], df['Like'])
crosstab = crosstab[['I hate it!-5','-4','-3','-2','-1','0','+1','+2','+3','+4','I love it!+5']]
crosstab

# calculating the mean of 'VisitFrequency'
df['VisitFrequency'] = LabelEncoder().fit_transform(df['VisitFrequency'])
visit = df.groupby('cluster_num')['VisitFrequency'].mean()
visit = visit.to_frame().reset_index()
visit

# calculating the mean of 'Like'
df['Like'] = LabelEncoder().fit_transform(df['Like'])
Like = df.groupby('cluster_num')['Like'].mean()
Like = Like.to_frame().reset_index()
Like

# calculating the mean of 'Gender'
df['Gender'] = LabelEncoder().fit_transform(df['Gender'])
Gender = df.groupby('cluster_num')['Gender'].mean()
Gender = Gender.to_frame().reset_index()
Gender

segment = Gender.merge(Like, on='cluster_num', how='left').merge(visit, on='cluster_num', how='left')
segment

# targeting segments
plt.figure(figsize = (9,4))
sns.scatterplot(x = "VisitFrequency", y = "Like",data=segment,s=400, color="r")
plt.title("Simple segment evaluation plot for the fast food data set", fontsize = 15)
plt.xlabel("Visit", fontsize = 12)
plt.ylabel("Like", fontsize = 12)
plt.show()

# Hierarchical Clustering
import scipy.cluster.hierarchy as shc

plt.figure(figsize=(12, 10))
plt.title("Customer Dendograms")
dend = shc.dendrogram(shc.linkage(dfEncoded, method='ward'))
plt.axhline(y = 10, color = 'r', linestyle = '-')
plt.show()

"""

*   Number of clusters given by dendograms is 4


"""

from sklearn.cluster import AgglomerativeClustering

cluster = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')
cluster.fit_predict(dfEncoded)

